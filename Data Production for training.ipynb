{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5314c831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5a840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(\"outputxNew.csv\").to_numpy()\n",
    "y_train = pd.read_csv(\"outputyNew.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94630595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets = pd.read_csv(\"tweets.csv\")\n",
    "# tweetI = pd.read_csv(\"tweet-index.csv\")\n",
    "df = pd.read_csv(\"AMZN.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bb09ed",
   "metadata": {},
   "source": [
    "# Preprocessing the dates and values, merging two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70dd18cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>37.896000</td>\n",
       "      <td>37.938000</td>\n",
       "      <td>37.384998</td>\n",
       "      <td>37.683498</td>\n",
       "      <td>37.683498</td>\n",
       "      <td>70422000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>37.919498</td>\n",
       "      <td>37.984001</td>\n",
       "      <td>37.709999</td>\n",
       "      <td>37.859001</td>\n",
       "      <td>37.859001</td>\n",
       "      <td>50210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>38.077499</td>\n",
       "      <td>39.119999</td>\n",
       "      <td>38.013000</td>\n",
       "      <td>39.022499</td>\n",
       "      <td>39.022499</td>\n",
       "      <td>116602000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>39.118000</td>\n",
       "      <td>39.972000</td>\n",
       "      <td>38.924000</td>\n",
       "      <td>39.799500</td>\n",
       "      <td>39.799500</td>\n",
       "      <td>119724000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>39.900002</td>\n",
       "      <td>40.088501</td>\n",
       "      <td>39.588501</td>\n",
       "      <td>39.846001</td>\n",
       "      <td>39.846001</td>\n",
       "      <td>68922000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close   \n",
       "0  2017-01-03  37.896000  37.938000  37.384998  37.683498  37.683498  \\\n",
       "1  2017-01-04  37.919498  37.984001  37.709999  37.859001  37.859001   \n",
       "2  2017-01-05  38.077499  39.119999  38.013000  39.022499  39.022499   \n",
       "3  2017-01-06  39.118000  39.972000  38.924000  39.799500  39.799500   \n",
       "4  2017-01-09  39.900002  40.088501  39.588501  39.846001  39.846001   \n",
       "\n",
       "      Volume  \n",
       "0   70422000  \n",
       "1   50210000  \n",
       "2  116602000  \n",
       "3  119724000  \n",
       "4   68922000  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b811303",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.post_date = tweets.post_date.astype(str)\n",
    "i = 0\n",
    "for tweet in tweets.post_date:\n",
    "    tweets.loc[i, 'post_date'] = datetime.utcfromtimestamp(int(tweets.loc[i, 'post_date'])).strftime('%Y-%m-%d')\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a08595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(tweets, tweetI, how='inner', on='tweet_id')\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('merged_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f10b2607",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedFile = pd.read_csv(\"merged_file.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1991b3e",
   "metadata": {},
   "source": [
    "# Normal Bert model that We initially decided to use for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "209ebf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashutoshsingh/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b84094",
   "metadata": {},
   "source": [
    "# FinBert model that we are currently using for first part of the problem i.e. Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94800e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ecd736c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lst):\n",
    "    \"\"\"\n",
    "    Flattens a 2D list into a 1D list.\n",
    "    \"\"\"\n",
    "    flattened = []\n",
    "    for sublist in lst:\n",
    "        for item in sublist:\n",
    "            flattened.append(item)\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be2ceb",
   "metadata": {},
   "source": [
    "# Here I am trying to create my own dataset on which I'll train my secon model which is an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c62d312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [\"2018-08-30\", \"2018-08-15\",\"2018-07-30\",\"2018-07-15\",\"2018-06-30\",\"2018-06-15\",\"2018-05-30\",\"2018-05-15\",\"2018-04-30\",\"2018-04-15\",\n",
    "\"2017-08-30\", \"2017-08-15\",\"2017-07-30\",\"2017-07-15\",\"2017-06-30\",\"2017-06-15\",\"2017-05-30\",\"2017-05-15\",\"2017-04-30\",\"2017-04-15\"]\n",
    "x_train = []\n",
    "y_train = []\n",
    "for targetDate in dates:\n",
    "    target_date = pd.to_datetime(targetDate)\n",
    "    one_week_before = target_date - timedelta(days=7)\n",
    "    one_week_before = one_week_before.strftime('%Y-%m-%d')\n",
    "    start_value = one_week_before\n",
    "    end_value = targetDate\n",
    "    # Use boolean indexing to select tuples that fall within the specified range\n",
    "    resultPart = mergedFile[(mergedFile['post_date'] >= start_value) & (mergedFile['post_date'] <= end_value)]\n",
    "    resultCompany = resultPart[resultPart['ticker_symbol'] == \"AMZN\"]\n",
    "    resultCompany = resultCompany.sort_values('retweet_num', ascending=False)\n",
    "    resultCompany = resultCompany.head(200)\n",
    "    # tweetsOb = result['body']\n",
    "    tweetsOb = resultCompany['body'].values\n",
    "    retweetsOb = resultCompany['retweet_num'].values\n",
    "    commentOb = resultCompany['comment_num'].values\n",
    "    likeOb = resultCompany['like_num'].values\n",
    "    \n",
    "    daysOb = [(datetime.strptime(targetDate, \"%Y-%m-%d\")-datetime.strptime(date2, \"%Y-%m-%d\")).days for date2 in resultCompany['post_date']]\n",
    "    \n",
    "    sentiResult = nlp(tweetsOb.tolist())\n",
    "    labels = [{\"positive\":0,\"neutral\":0,\"negative\":0} for i in sentiResult]\n",
    "    i =0\n",
    "    for obj in sentiResult:\n",
    "        (labels[i])[obj['label']] = obj['score']\n",
    "        i=i+1\n",
    "    positive = [elem['positive'] for elem in labels]\n",
    "    negative = [elem['negative'] for elem in labels]\n",
    "    neutral = [elem['neutral'] for elem in labels]\n",
    "    \n",
    "    labels = [int(i['label'].split()[0]) for i in sentimentResult]\n",
    "    xComp = [[a,b,c,d,e,f,g] for a,b,c,d,e,f,g in zip(positive, negative, neutral, retweetsOb, commentOb, likeOb, daysOb)]    \n",
    "#     xComp = flatten(xComp)\n",
    "\n",
    "    stockTargetDate = target_date + timedelta(days=1)\n",
    "    six_days_before = stockTargetDate - timedelta(days=6)\n",
    "    six_days_before = six_days_before.strftime('%Y-%m-%d')\n",
    "\n",
    "    start_value = six_days_before\n",
    "    end_value = stockTargetDate.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Use boolean indexing to select tuples that fall within the specified range\n",
    "    st = df[(df['Date'] >= start_value) & (df['Date'] <= end_value)]\n",
    "    diff = st.Close.tolist()[-1] - st.Open.tolist()[0]\n",
    "    delta = diff / st.Open.tolist()[0]\n",
    "    delta\n",
    "    for elem in xComp:\n",
    "        x_train.append(elem)\n",
    "        y_train.append(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f83b33d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b125d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4175c9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "\n",
    "xDf = pd.DataFrame(x_train, columns=['A', 'B', 'C','D','E','F','G'])\n",
    "yDf = pd.DataFrame(y_train.tolist(), columns=['values'])\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "xDf.to_csv('outputxNew.csv', index=False)\n",
    "yDf.to_csv('outputyNew.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bf4abc54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.920105</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.539348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>321.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.903385</td>\n",
       "      <td>59.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.904519</td>\n",
       "      <td>57.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.744008</td>\n",
       "      <td>55.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.902423</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.939989</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.551905</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.923244</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.943658</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             A    B         C     D     E      F    G\n",
       "0     0.000000  0.0  0.920105  88.0   4.0  125.0  1.0\n",
       "1     0.539348  0.0  0.000000  77.0   7.0  321.0  2.0\n",
       "2     0.000000  0.0  0.903385  59.0   8.0  147.0  1.0\n",
       "3     0.000000  0.0  0.904519  57.0  27.0   71.0  1.0\n",
       "4     0.000000  0.0  0.744008  55.0  17.0   73.0  1.0\n",
       "...        ...  ...       ...   ...   ...    ...  ...\n",
       "3995  0.000000  0.0  0.902423   2.0   1.0    6.0  3.0\n",
       "3996  0.000000  0.0  0.939989   2.0   0.0    0.0  1.0\n",
       "3997  0.000000  0.0  0.551905   2.0   1.0    4.0  6.0\n",
       "3998  0.000000  0.0  0.923244   2.0   0.0    7.0  7.0\n",
       "3999  0.000000  0.0  0.943658   2.0   2.0    4.0  3.0\n",
       "\n",
       "[4000 rows x 7 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b98b223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 20\n",
    "num_timesteps = 200\n",
    "num_features = 7\n",
    "\n",
    "reshaped_input_data = np.zeros((num_samples, num_timesteps, num_features))\n",
    "for i in range(num_samples):\n",
    "    reshaped_input_data[i] = x_train[i*num_timesteps:(i+1)*num_timesteps]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f992230",
   "metadata": {},
   "outputs": [],
   "source": [
    "newY_train = y_train[::200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "57b3aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(actual,predicted):\n",
    "    fails = 0;\n",
    "    passes = 0;\n",
    "    for i, j in zip(actual,predicted):\n",
    "        if(i*j<0):\n",
    "            fails=fails+1\n",
    "        else:\n",
    "            passes=passes+1\n",
    "    return passes/(passes+fails)\n",
    "\n",
    "# def Accuracy(actual,predicted):\n",
    "#     sums = 0\n",
    "#     for i, j in zip(actual,predicted):\n",
    "#         sums  = sums+abs(i-j)\n",
    "#     return sums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf41924",
   "metadata": {},
   "source": [
    "## LSTM for the second model that is the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87c15351",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'float'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Output layer with single node for regression\u001b[39;00m\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/engine/data_adapter.py:1082\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1079\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[1;32m   1081\u001b[0m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m-> 1082\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1083\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle input: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1084\u001b[0m             _type_name(x), _type_name(y)\n\u001b[1;32m   1085\u001b[0m         )\n\u001b[1;32m   1086\u001b[0m     )\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(adapter_cls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1089\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData adapters should be mutually exclusive for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1090\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandling inputs. Found multiple adapters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1091\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[1;32m   1092\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'float'>\"})"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# input data with shape (20, 200, 7)\n",
    "input_data = reshaped_input_data\n",
    "\n",
    "# target data with shape (20,)\n",
    "target_data = newY_train\n",
    "\n",
    "# LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(200, 7)))  # 200 time steps, 7 features\n",
    "model.add(Dense(1))  # Output layer with single node for regression\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(input_data, target_data, epochs=100, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3a86b460",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(reshaped_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c54dc9e1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mAccuracy\u001b[49m(pred,newY_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "Accuracy(pred,newY_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1372f8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f2ff3ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LSTM-Final.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
